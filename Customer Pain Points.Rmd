---
title: "Customer Pain Points"
author: "Lukas Lichtner & Andy Cao"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# packages we are using right now
library(tidyverse)
library(openxlsx)
library(visdat)


```

##Reading in the data
```{r Reading in the data}
dat <- read.xlsx("HybridmobilitaÌˆt systemized.xlsx", sheet = "Labels", startRow = 2)
```

We've renamed each variable name to a shorter one, and marked variables which are not necessary with "X_" as a prefix. As such, we can now filter all the variables which has this prefix.
```{r Filtering out not needed variables}
dat <- dat %>% 
   select(-starts_with("X_"))
```

Further, we've named each "importance" metric with "IMP", and each "satisfaction" metric with "SAT". As each metric gets asked twice (one for importance and one for satisfaction), we've labeled them with the same number after the IMP or SAT prefix (e.g. SAT_1 and IMP_1 correspond to the same metric). We can filter for them as well, with their "ID" columns so combining them again. 
```{r Filter for importance and satisfaction ratings}
IMP_SAT <- dat %>% select("ID",
                          starts_with("SAT"),
                          starts_with("IMP"))
```

Additionally, we can filter for demographic variables, which are stored in the first 70 columns
```{r Reading in the data}

demographics <- dat %>% select("iD",
                               1:70)
```

Lastly, we can filter for the PR1 data, which might add additional demographic data
```{r Filter for additional demographics data}
PR_ratings <- dat %>% select("ID",
                             starts_with("PR1"))
```
## Cleaning the data

Now that we have 3 different datasets, we can start with datacleaning each of these datasets sepparately. We'll start with the importance and satisfaction scores (which we will then use to calculate the "pain" points).  

As values are stored as "[number] = [interpretation]" in these metrics, and we only need the number, we extract the first character in these metrics and turn them into class numeric
```{r Extract first character from IMP_SAT}
IMP_SAT <- IMP_SAT %>% 
   mutate_at(.vars = vars(starts_with("SAT")),
             .funs = ~str_extract(string= ., pattern = "^.")) %>% 
   mutate_at(.vars = vars(starts_with("IMP")),
             .funs = ~str_extract(string= ., pattern = "^.")) %>% 
   mutate_if(.predicate = is.character,
             .funs = as.numeric)

IMP_SAT %>% head()
```

We can now calculate "paint point" scores for each of the 106 metrics, and to make this more efficient, we'll use pivot longer to have 3 columns: the first identifies whether it was a importance or a satisfaction rating, the second which metric number it was (1-106), and the values are the ratings themselves.
```{r Pivot longer IMP_SAT}
IMP_SAT_long <- IMP_SAT %>% 
   pivot_longer(cols = starts_with("IMP")| starts_with("SAT"),
                names_to = c("IMP/SAT","Metric_Nr"),
                values_to = "Rating",
                names_sep = "_")
IMP_SAT_long %>% head()
```

We'll have pivot this dataframe to a wider format (so we have IMP and SAT ratings, and metric number is still in a separate column) and calculate the differences for each metric and costumer, which functions as our pain point for each costumer.

```{r Pivot wider IMP_SAT and calculate pain points}
IMP_SAT_pain_points <- IMP_SAT_long %>% 
   pivot_wider(names_from = `IMP/SAT`,
               values_from = Rating) %>% 
   mutate(Diff = IMP-SAT)

IMP_SAT_pain_points %>% head()
```
Lastly, we can pivot this difference columns to a wider format again (so each person who filled out the survey has its own row again).
```{r}
Pain_points <- IMP_SAT_pain_points %>% 
   select(ID, Metric_Nr, Diff) %>% 
   mutate(Metric_Nr = as.numeric(Metric_Nr)) %>% 
   arrange(ID, Metric_Nr) %>% 
   pivot_wider(names_from = Metric_Nr,
               values_from = `Diff`)
   
```

We can now use kmeans clustering to get metric clusters. We will use a for loop to cluster the questions according to 2 to 10 clusters.  We then take the cluster where the F-value (ratio of between sum of squares and within sum of squares) is the largest. **Is this a good approach?** Are there alternatives?

```{r}
set.seed(42)
for (i in seq(from = 2, to = 10, by = 1)){
   k <- i
   
   km_fit <- kmeans(Pain_points[,-1], centers = k)
   
   N <- nrow(Pain_points)
   
   Fstat[i] <- (km_fit$betweenss / (k-1)) / (km_fit$tot.withinss / (N-k))
}

which(Fstat == max(Fstat))

```

In this case, the F-value is largest in the second for loop, which means a cluster of 3 different questions are 

```{r Cleaning the data}
rm(list = base::setdiff(ls(), "full_dat"))
```