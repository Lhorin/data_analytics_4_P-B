---
title: "Customer Pain Points"
author: "Lukas Lichtner & Andy Cao"
date: "`r Sys.Date()`"
output: 
   html_document:
      toc: true
      number_sections: false
      toc_float: true
      collapsed: true
      code_fold: show
      
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

# packages we are using right now
library(tidyverse)
library(openxlsx)
library(visdat)
library(corrplot)
library(caret)
library(glmnet)


```

# Data preparation  

## Reading in the data
```{r Reading in the data}
dat <- read.xlsx("Hybridmobilität systemized.xlsx", sheet = "Labels", startRow = 2)
```

We've renamed each variable name to a shorter one, and marked variables which are not necessary with "X_" as a prefix. As such, we can now filter all the variables which has this prefix.
```{r Filtering out not needed variables}
dat <- dat %>% 
   select(-starts_with("X_"))
```

Further, we've named each "importance" metric with "IMP", and each "satisfaction" metric with "SAT". As each metric gets asked twice (one for importance and one for satisfaction), we've labeled them with the same number after the IMP or SAT prefix (e.g. SAT_1 and IMP_1 correspond to the same metric). We can filter for them as well with their "ID" columns so we can combine them again at a later stage. 
```{r Filter for importance and satisfaction ratings}
IMP_SAT <- dat %>% select("ID",
                          starts_with("SAT"),
                          starts_with("IMP"))
```

Additionally, we can filter for demographic variables, which are stored in the first 52 columns.
```{r Reading in demographics}

demographics <- dat %>% select("ID",
                               1:52)
```

Lastly, we can filter for the Profiling Questions, which might add additional information to the participant.
```{r Filter for additional demographics data}
PR_ratings <- dat %>% select("ID",
                             starts_with("PR1"))
```
## Cleaning the data

Now that we have 3 different datasets, we can start with datacleaning each of these datasets sepparately. We'll start with the importance and satisfaction scores (which we will then use to calculate the "pain" points).

### Cleaning Importance and Satisfaction Scores

As values are stored as "[number] = [interpretation]" in these metrics, and we only need the number, we extract the first character in these metrics and turn them into class numeric.
```{r Extract first character from IMP_SAT}
IMP_SAT <- IMP_SAT %>% 
   mutate_at(.vars = vars(starts_with("SAT")),
             .funs = ~str_extract(string= ., pattern = "^.")) %>% 
   mutate_at(.vars = vars(starts_with("IMP")),
             .funs = ~str_extract(string= ., pattern = "^.")) %>% 
   mutate_if(.predicate = is.character,
             .funs = as.numeric)

```

We can now calculate "paint point" scores for each of the 106 metrics, and to make this more efficient, we'll use pivot longer to have 3 columns: the first identifies whether it was a importance or a satisfaction rating, the second which metric number it was (1-106), and the values are the ratings themselves.
```{r Pivot longer IMP_SAT}
IMP_SAT_long <- IMP_SAT %>% 
   pivot_longer(cols = starts_with("IMP")| starts_with("SAT"),
                names_to = c("IMP/SAT","Metric_Nr"),
                values_to = "Rating",
                names_sep = "_")
IMP_SAT_long %>% head()
```

We'll have to pivot this dataframe to a wider format (so we have IMP and SAT ratings in sepparate columns) and calculate the differences for each metric and costumer, which functions as our pain point for each costumer.

```{r Pivot wider IMP_SAT and calculate pain points}
IMP_SAT_pain_points <- IMP_SAT_long %>% 
   pivot_wider(names_from = `IMP/SAT`,
               values_from = Rating) %>% 
   mutate(Diff = IMP-SAT)

IMP_SAT_pain_points %>% head()
```
Lastly, we can pivot this dataframe to a wider format again (so each person who filled out the survey has its own row again).
```{r}
Pain_points <- IMP_SAT_pain_points %>% 
   select(ID, Metric_Nr, Diff) %>% 
   mutate(Metric_Nr = as.numeric(Metric_Nr)) %>% 
   arrange(ID, Metric_Nr) %>% 
   pivot_wider(names_from = Metric_Nr,
               values_from = `Diff`)
   
Pain_points[1:10, 1:10]
```


### Cleaning Demographics

In this section, we will clean the `demographics`.  
This is how our current demographic dataframe looks like:
```{r Visualize demographic dataset}
vis_dat(demographics, sort_type = FALSE)

```

First, we create a function which can detect dichotomous variables (e.g. cells contain "Ausgewählt" or "Nicht ausgewählt"). We store the column number in a vector as a means to recode them as numeric variables (0 or 1) later on.  

```{r Create yes_no vector}
is_yes_no_question <- function(x) {
   vec <- c()
   counter <- 1
   
   for (i in 1:ncol(x)) {
      if (x[1, i] == "Ausgewählt" |
          x[1, i] == "Nicht ausgewählt") {
         vec[counter] <-  i
         counter <- counter + 1
      }
   }
   return(vec)
}

yes_no_cols <- is_yes_no_question(demographics)
yes_no_cols
```

We then recode said variables to 1 and 0 respectively.

```{r Using said vector, recode to 1 and 0}
demographics <- demographics %>% 
   rowwise() %>% 
   mutate_at(.vars = yes_no_cols, 
             .funs = ~if_else(. == "Ausgewählt", 1, 0))
```

We'll then turn every character column as a factor.

```{r Create Factors out of character variables}
demographics <- demographics %>% mutate_if(is.character, factor)

```

Then we order the factors according to their proper order if it is ordinal scaled. This is a tedious step, as we have to look at each variable...

```{r Create Ordinal scaled factors if needed}
#levels: from smallest to biggest

demographics$transportation_pro_woche <-
   factor(demographics$transportation_pro_woche,
          levels = c("Mehrmals in der Woche",
                     "Täglich"))
#still unsure:
#demographics$prozent_berufstätig <- factor(demographics$prozent_berufstätig)

demographics$nutzung_oev <-
   factor(
      demographics$nutzung_oev,
      levels = c(
         "Einmal im Monat oder seltener",
         "Mehrmals im Monat",
         "Einmal in der Woche",
         "Mehrmals in der Woche",
         "Täglich"
      )
   )

demographics <- demographics %>%
   mutate_at(.vars = vars(starts_with("aw_")),
             .funs = ~ factor(
                . ,
                levels = c(
                   "Nie",
                   "Seltener",
                   "Mehrmals im Monat",
                   "Mehrmals pro Woche",
                   "(fast) Täglich"
                )
             ))

demographics <- demographics %>%
   mutate_at(.vars = vars(starts_with("fz_")),
             .funs = ~ factor(
                . ,
                levels = c(
                   "Nie",
                   "Seltener",
                   "Mehrmals im Monat",
                   "Mehrmals pro Woche",
                   "(fast) Täglich"
                )
             ))

demographics$neuen_ort <-
   factor(
      demographics$neuen_ort,
      levels = c(
         "Nie",
         "1 mal im Monat oder seltener",
         "2-3 mal im Monat",
         "4 mal im Monat oder häufiger"
      )
   )

demographics$privates_auto <-
   factor(
      demographics$privates_auto,
      levels = c(
         "Nein, ich habe keinen Zugang zu einem privaten Auto",
         "Ja, aber es gehört Bekannten/Freunden oder jemand aus der Familie in einem anderen Haushalt",
         "Ja, aber es gehört jemand im selben Haushalt",
         "Ja, und es gehört mir selbst"
      )
   )

demographics$ausflug_oev <-
   factor(
      demographics$ausflug_oev,
      levels = c(
         "Nie",
         "Seltener",
         "2-3 mal im Monat",
         "1 mal in der Woche oder häufiger"
      )
   )

demographics$störungen_oev <-
   factor(
      demographics$störungen_oev,
      levels = c(
         "Nie",
         "1-2 mal im letzten Monat",
         "3-5 mal im letzten Monat",
         "6 mal oder häufiger im letzten Monat"
      )
   )

demographics$störungen_strasse <-
   factor(
      demographics$störungen_strasse,
      levels = c(
         "Nie",
         "1-2 mal im letzten Monat",
         "3-5 mal im letzten Monat",
         "6 mal oder häufiger im letzten Monat"
      )
   )

demographics$technologische_entwicklung <-
   factor(
      demographics$technologische_entwicklung,
      levels = c(
         "Technologische Entwicklungen interessieren mich nicht wirklich, ausser sie bieten mir einen konkreten Nutzen im Alltag",
         "Ich warte meist ab, bis mir andere von ihren Erfahrungen erzählen, bevor ich neue Technologien ausprobiere",
         "Ich probiere gerne Neues aus, verlasse mich dabei aber auf Produkte von bekannten und etablierten Marken",
         "Ich bin oft einer/eine der ersten in meinem Umfeld, der/die neue Technologien, Apps oder Gadgets ausprobiert"
      )
   )
```

### Cleaning Profiling Questions

The last datacleaning step would be to clean the `PR_ratings` dataset. Luckily, the data are stored as "[number] = [meaning]" again, so we can just extract the first character (in this case always the number) and turn them to a numeric variable again, like we did in the `IMP_SAT` dataset.

```{r Clean PR dataset}
PR_ratings <- PR_ratings %>%
   mutate_at(
      .vars = vars(starts_with("PR")),
      .funs = ~ str_extract(string = ., pattern = "^.")
   ) %>%
   mutate_if(.predicate = is.character,
             .funs = as.numeric) %>% 
   mutate(ID = as.numeric(ID))

PR_ratings[1:6,1:6]
```
We can then combine the `demographics` dataset with the `PR_ratings` dataset, as both contains useful information from the customer.

```{r Combine demographics datasets}
demographics_full <- demographics %>% 
   mutate(ID = as.numeric(ID)) %>% 
   full_join(PR_ratings, by = "ID")
```

This is how the dataframe looks like:
```{r visualize demographics dataset}
vis_dat(demographics_full, sort_type = F)
```

As we can see, the factor variables are still not numeric (which we need for the clustering method). As we've ordered them according to their levels in the previous steps, we can now turn them into numeric variables as well.

```{r Turn ordinal factors to numeric and visualize again}
demographics_full <- demographics_full %>% mutate_if(is.factor, as.numeric)

vis_dat(demographics_full, sort_type = F)
```

# Analysis
After cleaning the data, we can now start to analyse the data. The analysis is split between two pipelines. In the first section, we tried to cluster our costumers and see whether there are "archetypes" for which we can then base our future implementations.  

In the second section, we tried to predict whether the top pain points can be predicted using only demographic data.

## Segmentation of Customers (First Analysis Pipeline)
We have to standardize the whole dataframe first (except the ID variables) because the scales were not the same for each variable.

```{r standardize demographics df}
demog_scaled <- demographics_full %>% 
   select(-ID) %>% 
   scale(scale = TRUE)
```

As we still have some missing values in some cases, we'll try and impute them according to the variables median. **Are there other/ better ways?**

```{r Impute missing values with the mode}
repl_na_with_median <- function(x){
   mode_val <- median(x, na.rm = TRUE)
   x[is.na(x)] <- mode_val
   return(x)
}


vec_miss <- c()
for (i in 1:ncol(demog_scaled)){
   if (sum(is.na(demog_scaled[,i]) > 0)){
      vec_miss <- c(vec_miss, i)
   }
}


demog_scaled_imputed <- demog_scaled %>% 
   as.data.frame() %>% 
   mutate_at(.vars = vec_miss, .funs = repl_na_with_median) %>% 
   as.matrix()

```


Now we can try and cluster the demographics data. We'll try and "find" the best cluster size with a for loop, which then calculates the F-statistic for each cluster. The F-statistic is a ratio of between cluster sum of squares and within cluster sum of squares. In other words, we get a numeric value where we have a good fit between between and within cluster distances. We'll take the cluster number which has the biggest value. **Is this a good approach?**

```{r cluster with kmeans}

set.seed(42)
Fstat <- c()
for (i in seq(from = 2, to = 10, by = 1)){
   k <- i
   
   km_fit <- kmeans(demog_scaled_imputed, centers = k)
   
   N <- nrow(Pain_points)
   
   Fstat[i-1] <- (km_fit$betweenss / (k-1)) / (km_fit$tot.withinss / (N-k))
}

plot(seq(from = 2, to = 10, by = 1), y = Fstat)
```

In this case, the F-value is largest with 2 clusters.  

We'll create a new variable `group` with the clusters.

```{r create group variable with clusters}
km_fit <- kmeans(demog_scaled_imputed, centers = 2)

demographics_full$group <- km_fit$cluster
```

Let's try and visualize mean Importance and Satisfaction scores according to the two different clusters!

```{r visualize IMP and SAT according to clusters}
IMP_SAT_pain_points <- demographics_full %>%
   select(ID, group) %>% 
   full_join(IMP_SAT_pain_points, by = "ID")
   
IMP_SAT_pain_points %>% 
   group_by(group,Metric_Nr) %>% 
   summarize(mean_IMP = mean(IMP),
             mean_SAT = mean(SAT)) %>% 
   ggplot(aes(mean_IMP, mean_SAT))+
   geom_label(aes(label = Metric_Nr),
              label.padding = unit(.05, "lines"),
              size = 3)+
   facet_wrap(~group)+
   theme_bw()
   
```

## Predicting Pain Point Variability with Demographics

In this section, we try a separate approach in first identifying the top 10 pain points out of the 106 metrics, and then predict with only the demographic data.  

Let us start with picking the top X pain points first.  

As the results and interpretation can quite differ for different approaches, we try to first filter for all the metrics which has a mean importance value above 3.5 (still arbitrarily set). In this way, we make sure that the resulting metrics are important to the customers, but are unsatisfied with the current state (thus the pain point is large). Otherwise, we would risk in having metrics coming out on top which are not of concern to the costumers.  

Here are the resulting metrics, sorted my their mean importance
```{r filter for mean importance above 3.5, and sort it}
mean_imp <- IMP_SAT_pain_points %>% 
   group_by(Metric_Nr) %>% 
   summarize(mean_importance = mean(IMP)) %>% 
   filter(mean_importance >= 3.5) %>% 
   arrange(desc(mean_importance))

mean_imp
```

We can now filter for these specific metrics, and calculate the mean pain point (that is, the mean difference between importance and satisfaction rating) and plot it.

```{r given these metrics, calculate mean pain point and plot}
metrics_to_take <- mean_imp %>% pull(Metric_Nr) %>% as.numeric()

top_15_Pain_points <- IMP_SAT_pain_points %>% 
   filter(Metric_Nr %in% metrics_to_take) %>% 
   group_by(Metric_Nr) %>% 
   summarize(`Mean Pain Points` = mean(Diff)) %>% 
   slice_max(order_by = `Mean Pain Points`, n = 15)

top_15_Pain_points$Metric_Nr <- reorder(top_15_Pain_points$Metric_Nr, X = top_15_Pain_points$`Mean Pain Points`)

top_15_Pain_points %>%
   ggplot(aes(Metric_Nr, y = `Mean Pain Points`))+
   geom_segment(aes(x = Metric_Nr, xend = Metric_Nr, y = 0, yend = `Mean Pain Points`))+
   geom_point(color = "#28C2B9", size = 4, alpha = .9)+
   theme_minimal()+
   labs(x = "Metric Number", title = "Top 15 Pain Points in Decreasing Order")+
   coord_flip()+
   theme(panel.grid.minor.y = element_blank(),
         panel.grid.major.y = element_blank())+
   ylim(0,1)

```

According to this plot, the top 6 metrics have higher pain points, whereas the 7th to 11th and 12th to 15th metrics are one step lower. So we will take the top 6 metrics from here on out.  

Here are the top 6 metrics:  

```{r take top 6 metrics}
top_6_metrics <- top_15_Pain_points %>% 
   slice_max(order_by = `Mean Pain Points`, n = 6)

DV_set <- IMP_SAT_pain_points %>% 
   filter(Metric_Nr %in% top_6_metrics$Metric_Nr)

metric_labels <- read.xlsx("Hybridmobilität systemized.xlsx", sheet =3, startRow =1,colNames = F) %>% 
   mutate()

colnames(metric_labels) <- c("Metric_Nr", "Label")

DV_set <- DV_set %>% 
   mutate(Metric_Nr = as.numeric(Metric_Nr)) %>%
   left_join(metric_labels, by = "Metric_Nr")

unique(DV_set$Label)

```

As we have 82 demographic and profiling variables to choose from for the regularized prediction, it is better to choose the variables a priori before tucking them into the model.  

Though we are not quite sure how biased this approach is, one could view a correlation plot and just select those variables which have higher correlation with others, so we can make sure not to include redundant variables.  

```{r corrplot of profiling questions}
corrplot(cor(PR_ratings))
```

From this plot above, we see that most of the correlations are small or not present. But each profiling question has its subgroup (seen by the smaller squares). We'll choose from each profiling question the ones which has the "biggest" correlations.  

```{r select profiling questions}
selected_PR <- PR_ratings %>% select(ID, PR12_1, PR12_3, PR13_1, PR13_6, PR14_7, PR15_1)
```

For the demographics, we'll pick the generic ones (like age, gender, language), and handpick those that seem likely to be predictive of the metrics.  

```{r select demographics}
selected_demog <- demographics %>% select(ID, sprache, alter, geschlecht, prozent_berufstätig, wohnort, nutzung_oev, sbb_app, google_maps, auto_indiv, neuen_ort, privates_auto, aw_auto, aw_zug, aw_bus, aw_tram, fz_auto, fz_zug, fz_bus, fz_tram, car_sharing, störungen_oev) %>% 
   mutate(ID = as.numeric(ID))

```

Let's put everything together! 
```{r combine datasets and split into test/ train sets}
prediction_df <- DV_set %>% 
   select(ID, Diff, Label) %>% 
   pivot_wider(names_from = Label, values_from = Diff) %>% 
   full_join(selected_demog, by = "ID") %>% 
   full_join(selected_PR, by = "ID")



DV <- prediction_df %>% select(2:7)
IV <- prediction_df %>% select(8:ncol(prediction_df))
```

One last check whether the missing data is still present (maybe we had luck and picked variables without missing data):
```{r visualize IV and missingness}
vis_dat(IV, sort_type = F)
```

Sadly, the variables of interest have missing values. It seems like there is a pattern though... Let's see whether we can identify why missingness occured in our dataset. Only the questions where the customer gets asked how they commute to work is missing. Maybe it has to do with their current occupation?

```{r}
prediction_df %>% filter(is.na(aw_zug)) %>% 
   select(prozent_berufstätig, aw_zug, nutzung_oev)
```

It seems like these people are not in the work force right now, and thus have left the questions concerning "Arbeitsweg" unanswered. If I may, I'll impute "Nie" in these variables so we still work with all the customers.

```{r}
IV[is.na(IV)] <- "Nie"
vis_dat(IV, sort_type = F)
```

In the last step, we try and predict each dependent variable (in this case the pain points for each customer for the 6 metrics) separately with the selected demographic and profiling data. For this, we'll use regularized regression and try to use 10-fold cross validation so we make sure we get out-of sample performance and see whether our model is generalizable or not.  

```{r}
set.seed(42)
train_ctr <- trainControl(method = "cv", number = 10)

model <-
   train(
      x = IV, 
      y = DV$`so früh wie möglich zu wissen, welche alternative Route man nehmen kann, wenn eine Störung oder Verspätung eintritt`,
      method = "glmnet",
      metric = "RMSE",
      preProcess = c("center", "scale"),
      trControl = train_ctr
   )

print(model)
```


```{r}
train_ctr <- trainControl(method = "cv", number = 10)

model <-
   train(
      x = IV, 
      y = DV$`so genau wie möglich zu wissen, was los ist, wenn ein Zug nicht mehr fährt`,
      method = "glmnet",
      metric = "RMSE",
      preProcess = c("center", "scale"),
      trControl = train_ctr
   )

print(model)
```

```{r}
train_ctr <- trainControl(method = "cv", number = 10)

model <-
   train(
      x = IV, 
      y = DV$`so schnell wie möglich das Ausmass einer Störung abschätzen zu können; z.B. fällt nur 1 Tram aus, ist der ganze öV betroffen etc.`,
      method = "glmnet",
      metric = "RMSE",
      preProcess = c("center", "scale"),
      trControl = train_ctr
   )

print(model)
```

```{r}
train_ctr <- trainControl(method = "cv", number = 10)

model <-
   train(
      x = IV, 
      y = DV$` immer zum günstigsten Tarif zu fahren`,
      method = "glmnet",
      metric = "RMSE",
      preProcess = c("center", "scale"),
      trControl = train_ctr
   )

print(model)
```

```{r}
train_ctr <- trainControl(method = "cv", number = 10)

model <-
   train(
      x = IV, 
      y = DV$` nie einen falschen Zug zu nehmen`,
      method = "glmnet",
      metric = "RMSE",
      preProcess = c("center", "scale"),
      trControl = train_ctr
   )

print(model)
```

```{r}
train_ctr <- trainControl(method = "cv", number = 10)

model <-
   train(
      x = IV, 
      y = DV$` so früh wie möglich im Voraus zu wissen, ob man einen Anschlusszug erwischen wird oder nicht, wenn man Verspätung hat`,
      method = "glmnet",
      metric = "RMSE",
      preProcess = c("center", "scale"),
      trControl = train_ctr
   )

print(model)
```

So we can't predict whether it is a pain point or not.... only 2.7% - 10% can be explained.

